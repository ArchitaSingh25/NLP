{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9Sicyw_ctyZL"
      },
      "outputs": [],
      "source": [
        "import nltk                               #package to work with human language data\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize   #refer https://www.nltk.org/api/nltk.tokenize.html\n",
        "from nltk.corpus import stopwords         #refer https://www.geeksforgeeks.org/removing-stop-words-nltk-python/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words=set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8gbG5cHwB3V",
        "outputId": "2cdb056e-490a-4321-b04f-7662cb18bebb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# methods for cleaning the sentence\n",
        "def extract_words(sentence):\n",
        "  words=nltk.word_tokenize(sentence)\n",
        "  cleaned_text=[w.lower() for w in words if w not in stop_words]\n",
        "  return cleaned_text\n"
      ],
      "metadata": {
        "id": "M1mdYgCrwwQo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#method to build vocabulary\n",
        "def tokenize_sentence(sentences):\n",
        "  words=[]\n",
        "  for sentence in sentences:\n",
        "    w=extract_words(sentence)\n",
        "    words.extend(w)\n",
        "  words = sorted(list(set(words)))  #set fn will return distinct values\n",
        "  return words"
      ],
      "metadata": {
        "id": "41e02kVTxfvR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To build a bag of words ie word count vector\n",
        "def bagOfWords(sentence,words):\n",
        "  sentence_words=extract_words(sentence)\n",
        "\n",
        "  #frequency word count\n",
        "  bag=np.zeros(len(words))\n",
        "  for sw in sentence_words:\n",
        "    for i,words in enumerate(words):\n",
        "      if words==sw:\n",
        "        bag[i]+=1\n",
        "\n",
        "  return np.array(bag)"
      ],
      "metadata": {
        "id": "BJnRGNhOyUJ9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=[\"John saw the train\",\n",
        "        \"The train was late\",\n",
        "        \"Max and Rob took the word\",\n",
        "        \"I looked for Max and Rob at the bus station\",\n",
        "        \"Max and Rob arrived at the bus station early but waited until noon for the bus\"]\n",
        "\n",
        "vocabulary= tokenize_sentence(corpus)\n",
        "print(\"Vocubulary \\n\",vocabulary,\"\\n\")\n",
        "\n",
        "for i in corpus:\n",
        "  print(i,\" \\nWord Count Array:\" ,bagOfWords(i,vocabulary),\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBNR2SKY0Dq2",
        "outputId": "2dab7a96-8ed3-4441-ec1d-74f8b9a3df97"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocubulary \n",
            " ['arrived', 'bus', 'early', 'i', 'john', 'late', 'looked', 'max', 'noon', 'rob', 'saw', 'station', 'the', 'took', 'train', 'waited', 'word'] \n",
            "\n",
            "John saw the train  \n",
            "Word Count Array: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
            "\n",
            "The train was late  \n",
            "Word Count Array: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] \n",
            "\n",
            "Max and Rob took the word  \n",
            "Word Count Array: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
            "\n",
            "I looked for Max and Rob at the bus station  \n",
            "Word Count Array: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
            "\n",
            "Max and Rob arrived at the bus station early but waited until noon for the bus  \n",
            "Word Count Array: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
            "\n"
          ]
        }
      ]
    }
  ]
}